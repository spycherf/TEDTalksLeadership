from datetime import datetime
import json
import numpy as np
import pandas as pd

TED = "data/metadata_transcripts/ted/output/ted_talks_TED_metadata_transcripts.csv"
YT_MD = "data/metadata_transcripts/youtube/output/ted_talks_YT_metadata.csv"
YT_TRANSCRIPTS = "data/metadata_transcripts/youtube/output/ted_talks_YT_transcripts.csv"
IBM = "data/personality_profiles/output/personality_profiles_json.csv"
AZURE = "data/gender_age/azure/output/gender_age_estimates.csv"
GMM = "data/gender_age/gmm/output/gender_estimates.csv"


def load_merge():
    ted = pd.read_csv(TED)
    yt_metadata = pd.read_csv(YT_MD)
    yt_transcripts = pd.read_csv(YT_TRANSCRIPTS)
    ibm = pd.read_csv(IBM)
    azure = pd.read_csv(AZURE)
    gmm = pd.read_csv(GMM)

    df = ted.merge(yt_metadata, how="left", left_on="ext_id", right_on="id", suffixes=("", "_YT"))
    df = df.merge(yt_transcripts, how="left", left_on="id_YT", right_on="yt_id", suffixes=("", "_YT"))
    df = df.merge(ibm, how="left", left_on="id", right_on="id", suffixes=("", "_IBM"))
    df = df.merge(gmm, how="left", left_on="id", right_on="id", suffixes=("", "_GMM"))
    df = df.merge(azure, how="left", left_on="id", right_on="id", suffixes=("", "_AZURE"))

    # Remove any duplicates
    df = df.drop_duplicates(subset="id", keep="first")
    
    return df


def transcript_summary(df):
    english = (df["native_language"] == "en")
    autogenerated = (df["autogenerated"] == 1)
    translated = (df["translated"] == 1)
    ted_transcript = (~df["transcript"].isna())
    yt_transcript = (~df["transcript_YT"].isna())

    print("ENGLISH TALKS:", df[english].shape[0])
    print("  w/o transcript:", df[english & ~(ted_transcript | yt_transcript)].shape[0])
    print("  w/ transcript:", df[english & (ted_transcript | yt_transcript)].shape[0])
    print("    from TED:", df[english & ted_transcript].shape[0])
    print("    from YouTube:", df[english & ~ted_transcript & yt_transcript].shape[0])
    print("      manually created:",
          df[english & ~ted_transcript & yt_transcript & ~autogenerated & ~translated].shape[0])
    print("      autogenerated (English):",
          df[english & ~ted_transcript & yt_transcript & autogenerated & ~translated].shape[0])
    print("      translated (Google):",
          df[english & ~ted_transcript & yt_transcript & ~autogenerated & translated].shape[0])
    print("      autogenerated then translated:",
          df[english & ~ted_transcript & yt_transcript & autogenerated & translated].shape[0])

    print("-----")

    print("NON-ENGLISH TALKS:", df[~english].shape[0])
    print("  w/o transcript:", df[~english & ~(ted_transcript | yt_transcript)].shape[0])
    print("  w/ English transcript:", df[~english & (ted_transcript | yt_transcript)].shape[0])
    print("    from TED:", df[~english & ted_transcript].shape[0])
    print("    from YouTube:", df[~english & ~ted_transcript & yt_transcript].shape[0])
    print("      manually created:",
          df[~english & ~ted_transcript & yt_transcript & ~autogenerated & ~translated].shape[0])
    print("      autogenerated (English):",
          df[~english & ~ted_transcript & yt_transcript & autogenerated & ~translated].shape[0])
    print("      translated (Google):",
          df[~english & ~ted_transcript & yt_transcript & ~autogenerated & translated].shape[0])
    print("      autogenerated then translated:",
          df[~english & ~ted_transcript & yt_transcript & autogenerated & translated].shape[0])

    print("-----")

    print("TOTAL:", df.shape[0])


def clean(df, remove_autogenerated, remove_empty_popularity, remove_empty_big5, export):
    if "yt_ids" in export:
        df[~df["ext_id"].isna()]["ext_id"].to_csv("youtube_ids.txt", header=False, index=False)

    if "ted_yt_ids" in export:
        df[~df["ext_id"].isna()][["id", "ext_id"]].to_csv("youtube_ids.csv", index=False)

    if "uncleaned" in export:
        df.to_csv("TED_unclean_FULL.csv", index=False)

    # Adapt data types
    df["duration"] = pd.Series(df["duration"], dtype="Int64")
    df["ext_duration"] = pd.Series(df["ext_duration"], dtype="Int64")
    df["date_recorded"] = pd.to_datetime(df["date_recorded"])
    df["date_published"] = pd.to_datetime(df["date_published"])
    df["date_published_YT"] = pd.to_datetime(df["date_published_YT"])
    df["likes"] = pd.Series(df["likes"], dtype="Int64")
    df["dislikes"] = pd.Series(df["dislikes"], dtype="Int64")
    df["views_YT"] = pd.Series(df["views_YT"], dtype="Int64")
    df["nb_comments_YT"] = pd.Series(df["nb_comments_YT"], dtype="Int64")
    df["autogenerated"] = pd.Series(df["autogenerated"], dtype="Int64")
    df["translated"] = pd.Series(df["translated"], dtype="Int64")
    df["est_age"] = pd.Series(df["est_age"], dtype="Int64")

    # Delete records with transcript of bad or dubious quality
    english = (df["native_language"] == "en")
    autogenerated = (df["autogenerated"] == 1)
    translated = (df["translated"] == 1)
    ted_transcript = ~df["transcript"].isna()
    yt_transcript = ~df["transcript_YT"].isna()
    df = df[~(english & ~ted_transcript & yt_transcript & autogenerated & translated) |
            ~(~english & ~ted_transcript & yt_transcript & autogenerated & ~translated)]

    if remove_autogenerated:
        df = df[~(autogenerated | df["autogenerated"].isna())]

    # Compute YouTube popularity score
    df["elapsed_time_YT"] = (datetime.fromtimestamp(1588204800) - df["date_published_YT"]).dt.total_seconds() / (
            3600 * 24)  # days between publication on YouTube and April 30 2020
    df["popularity_score"] = np.log(np.sqrt(df["likes"] + 1 / (df["dislikes"] + 1)) * df["views_YT"]) / (
            1 + np.log(df["elapsed_time_YT"]) / np.sqrt(np.log(df["elapsed_time_YT"])))
    df["popularity_score"] -= df["popularity_score"].min()  # min-max scaling
    df["popularity_score"] /= df["popularity_score"].max()  # min-max scaling

    if remove_empty_popularity:
        df = df[~df["popularity_score"].isna()]
        
    # Combine columns/replace null values
    df["duration"] = np.where(df["duration"] == 0, df["ext_duration"], df["duration"])

    df["views_YT"].fillna(0, inplace=True)
    df["combined_views_TED_YT"] = df["views"] + df["views_YT"]

    df["nb_comments_YT"].fillna(0, inplace=True)
    df["nb_comments"] = np.where(df["nb_comments"] == -1, 0, df["nb_comments"])
    df["combined_comments_TED_YT"] = df["nb_comments"] + df["nb_comments_YT"]

    df["description"] = df["description"].fillna(df["description_YT"])
    df["tags"] = df["tags"].fillna(df["tags_YT"])
    df["transcript"] = df["transcript"].fillna(df["transcript_YT"])

    df["est_age"].fillna(0, inplace=True)
    
    # Get rid of unnecessary columns
    df.drop(["nb_languages", "views", "nb_comments", "nb_speakers", "speakers", "speakers_desc",
             "ext_src", "ext_id", "ext_duration",
             "channel", "title_YT", "description_YT", "tags_YT", "views_YT", "nb_comments_YT", "elapsed_time_YT",
             "yt_id", "translated", "src_lang", "transcript_YT",
             "yt_id_GMM"
             ], axis=1, inplace=True)

    # Rename some columns
    df.rename(columns={"url": "url_TED",
                       "full_name": "talk_full_name",
                       "date_published": "date_published_TED",
                       "native_language": "language",
                       "likes": "likes_YT",
                       "dislikes": "dislikes_YT",
                       "est_gender": "predicted_gender_GMM",
                       "est_gender_AZURE": "predicted_gender_AZURE",
                       "est_age": "predicted_age_AZURE"
                       }, inplace=True)

    # Split into two dataframes: English talks with or without transcript
    english = (df["language"] == "en")
    has_transcript = (~df["transcript"].isna())

    df_notranscript = df[english & ~has_transcript]
    df = df[english & has_transcript]

    # Remove records with transcript shorter than 100 words
    df = df[df["transcript"].str.split().str.len() >= 100]

    # Unpack Big Five personality scores
    df["big5_O"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][0]["percentile"])
    df["big5_C"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][1]["percentile"])
    df["big5_E"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][2]["percentile"])
    df["big5_A"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][3]["percentile"])
    df["big5_N"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][4]["percentile"])

    if remove_empty_big5:
        df = df[~df["profile"].isna()]

    df.drop("profile", axis=1, inplace=True)

    # Reorder columns
    df = df[["id", "id_YT", "url_TED", "main_speaker", "title", "talk_full_name",
             "event", "event_type", "description", "tags",
             "date_recorded", "date_published_TED", "date_published_YT",
             "duration", "language",
             "transcript", "autogenerated",
             "combined_views_TED_YT", "combined_comments_TED_YT", "likes_YT", "dislikes_YT", "popularity_score",
             "predicted_gender_GMM", "predicted_gender_AZURE", "predicted_age_AZURE",
             "big5_O", "big5_C", "big5_E", "big5_A", "big5_N"
             ]]

    # Export English transcripts (for IBM tool)
    if "transcripts" in export:
        df[["id", "transcript"]].to_csv("transcripts.csv", index=False)

    # Export clean dataset
    if "cleaned" in export:
        df.to_csv("TED_clean_FULL.csv", index=False)

    df.reset_index(drop=True, inplace=True)
    df_notranscript.reset_index(drop=True, inplace=True)

    # Cleaning report
    print("ENGLISH TALKS (post-cleaning):", df.shape[0] + df_notranscript.shape[0])
    print("  w/ transcript:", df.shape[0])
    print("  w/o transcript:", df_notranscript.shape[0])

    return df, df_notranscript
