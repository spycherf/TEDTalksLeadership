from datetime import datetime
import json
import numpy as np
import pandas as pd


def transcript_summary(df):
    english = (df["native_language"] == "en")
    autogenerated = (df["autogenerated"] == 1)
    translated = (df["translated"] == 1)
    ted_transcript = (~df["transcript_TED"].isna())
    yt_transcript = (~df["transcript_YT"].isna())

    print("ENGLISH TALKS:", df[english].shape[0])
    print("  w/o transcript:", df[english & ~(ted_transcript | yt_transcript)].shape[0])
    print("  w/ transcript:", df[english & (ted_transcript | yt_transcript)].shape[0])
    print("    from TED:", df[english & ted_transcript].shape[0])
    print("    from YouTube:", df[english & ~ted_transcript & yt_transcript].shape[0])
    print("      manually created:",
          df[english & ~ted_transcript & yt_transcript & ~autogenerated & ~translated].shape[0])
    print("      autogenerated (English):",
          df[english & ~ted_transcript & yt_transcript & autogenerated & ~translated].shape[0])
    print("      translated (Google):",
          df[english & ~ted_transcript & yt_transcript & ~autogenerated & translated].shape[0])
    print("      autogenerated then translated:",
          df[english & ~ted_transcript & yt_transcript & autogenerated & translated].shape[0])

    print("-----")

    print("NON-ENGLISH TALKS:", df[~english].shape[0])
    print("  w/o transcript:", df[~english & ~(ted_transcript | yt_transcript)].shape[0])
    print("  w/ English transcript:", df[~english & (ted_transcript | yt_transcript)].shape[0])
    print("    from TED:", df[~english & ted_transcript].shape[0])
    print("    from YouTube:", df[~english & ~ted_transcript & yt_transcript].shape[0])
    print("      manually created:",
          df[~english & ~ted_transcript & yt_transcript & ~autogenerated & ~translated].shape[0])
    print("      autogenerated (English):",
          df[~english & ~ted_transcript & yt_transcript & autogenerated & ~translated].shape[0])
    print("      translated (Google):",
          df[~english & ~ted_transcript & yt_transcript & ~autogenerated & translated].shape[0])
    print("      autogenerated then translated:",
          df[~english & ~ted_transcript & yt_transcript & autogenerated & translated].shape[0])

    print("-----")

    print("TOTAL:", df.shape[0])


def clean(df, export=False):
    # Changing data types
    df["duration"] = pd.Series(df["duration"], dtype="Int64")
    df["ext_duration"] = pd.Series(df["ext_duration"], dtype="Int64")
    df["date_recorded"] = pd.to_datetime(df["date_recorded"])
    df["date_published_TED"] = pd.to_datetime(df["date_published_TED"])
    df["date_published_YT"] = pd.to_datetime(df["date_published_YT"])
    df["likes"] = pd.Series(df["likes"], dtype="Int64")
    df["dislikes"] = pd.Series(df["dislikes"], dtype="Int64")
    df["views_YT"] = pd.Series(df["views_YT"], dtype="Int64")
    df["nb_comments_YT"] = pd.Series(df["nb_comments_YT"], dtype="Int64")
    df["autogenerated"] = pd.Series(df["autogenerated"], dtype="Int64")
    df["translated"] = pd.Series(df["translated"], dtype="Int64")

    # Deleting records with transcripts of bad or dubious quality
    english = (df["native_language"] == "en")
    autogenerated = (df["autogenerated"] == 1)
    translated = (df["translated"] == 1)
    ted_transcript = ~df["transcript_TED"].isna()
    yt_transcript = ~df["transcript_YT"].isna()
    df = df[~(english & ~ted_transcript & yt_transcript & autogenerated & translated) |
            ~(~english & ~ted_transcript & yt_transcript & autogenerated & ~translated)]

    # Computing YouTube popularity score
    df["elapsed_time_YT"] = (datetime.fromtimestamp(1588204800) - df["date_published_YT"]).dt.total_seconds() / (
            3600 * 24)  # days between publication on YouTube and April 30 2020
    df["yt_popularity_score"] = np.log(np.sqrt(df["likes"] + 1 / (df["dislikes"] + 1)) * df["views_YT"]) / (
            1 + np.log(df["elapsed_time_YT"]) / np.sqrt(np.log(df["elapsed_time_YT"])))
    # df["yt_popularity_score"] -= df["yt_popularity_score"].min()  # min-max scaling
    # df["yt_popularity_score"] /= df["yt_popularity_score"].max()  # min-max scaling

    # Combining columns/replacing null values
    df["duration"] = np.where(df["duration"] == 0, df["ext_duration"], df["duration"])

    df["views_YT"].fillna(0, inplace=True)
    df["combined_views"] = df["views_TED"] + df["views_YT"]

    df["nb_comments_YT"].fillna(0, inplace=True)
    df["nb_comments_TED"] = np.where(df["nb_comments_TED"] == -1, 0, df["nb_comments_TED"])
    df["combined_comments"] = df["nb_comments_TED"] + df["nb_comments_YT"]

    df["description"] = df["description_TED"].fillna(df["description_YT"])
    df["tags"] = df["tags_TED"].fillna(df["tags_YT"])
    df["transcript"] = df["transcript_TED"].fillna(df["transcript_YT"])
    
    # Getting rid of unnecessary columns
    df.drop(["url", "description_TED", "tags_TED", "nb_languages",
             "views_TED", "nb_comments_TED", "nb_speakers", "speakers", "speakers_desc",
             "ext_src", "ext_id", "ext_duration", "transcript_TED",
             "channel", "title_YT", "description_YT", "tags_YT",
             "views_YT", "nb_comments_YT", "elapsed_time_YT",
             "yt_id", "translated", "src_lang", "transcript_YT", "id"
             ], axis=1, inplace=True)

    # Renaming some columns
    df.rename(columns={"id_TED": "id", "title_TED": "title",
                       "native_language": "language", "id_YT": "yt_id"
                       }, inplace=True)

    # Removing duplicates
    df = df.drop_duplicates(subset="id", keep="first")

    # Splitting into two dataframes: English talks with or without transcript
    english = (df["language"] == "en")
    has_transcript = (~df["transcript"].isna())
    df_notranscript = df[english & ~has_transcript]

    df = df[english & has_transcript]

    # Removing records with transcript shorter than 100 words
    df = df[df["transcript"].str.split().str.len() >= 105]

    # Unpacking personality profile information
    df["big5_O"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][0]["percentile"])
    df["big5_C"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][1]["percentile"])
    df["big5_E"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][2]["percentile"])
    df["big5_A"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][3]["percentile"])
    df["big5_N"] = df[~df["profile"].isna()]["profile"].apply(json.loads).apply(lambda x: x["personality"][4]["percentile"])
    df.drop("profile", axis=1, inplace=True)
    
    # Exporting CSV files, which can be used as inputs for other scripts (e.g. IBM or speech-to-text)
    if export:
        transcripts = df[["id", "transcript"]]
        transcripts.to_csv("transcripts.csv", index=False)

        criteria1 = (df_notranscript["yt_id"].isna())
        criteria2 = (df_notranscript["language"] == "en")
        notranscript = df_notranscript[~criteria1 & criteria2][["id", "yt_id"]]
        notranscript.to_csv("notranscript_yt_ids.csv", index=False)

    df.reset_index(drop=True, inplace=True)
    df_notranscript.reset_index(drop=True, inplace=True)

    return df, df_notranscript
